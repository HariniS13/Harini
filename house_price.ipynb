import pandas as pd 
import numpy as np 
import matplotlib.pyplot as plt 
import warnings 
warnings.filterwarnings('ignore') 
from sklearn.model_selection import train_test_split 
from sklearn.preprocessing import LabelEncoder 
from sklearn import metrics 
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score 
from sklearn.svm import SVR 
from sklearn.tree import DecisionTreeRegressor 
from sklearn.ensemble import RandomForestRegressor 
from sklearn import neighbors 
from math import sqrt 
%matplotlib inline 
df1 = pd.read_csv('data.csv') 
df1 
df1.head() 
df1.describe() 
df1.shape 
df1.ndim 
df1 = df1.dropna(thresh=0.70*len(df1),axis=1) 
X = X.fillna(df1.mean()) 
y = y.fillna(df1.mean()) 
corr = df1.corr() 
print (corr['price'].sort_values(ascending=False)[:5], '\n') 
print (corr['price'].sort_values(ascending=False)[-5:]) 
df1.isnull().sum() 
df1.isnull().mean() 
df1 
def missing (df): 
    missing_number = df1.isnull().sum().sort_values(ascending=False) 
    missing_percent = ((df1.isnull().sum()/df1.isnull().count())*100).sort_values(ascending=False) 
    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent']) 
    return missing_values 
total = df1.isnull().sum().sort_values(ascending = False) 
percent = (df1.isnull().sum()/df1.isnull().count()).sort_values(ascending = False) 
missing_data = pd.concat([total,percent], axis = 1, keys = ['Total', 'Percent']) 
missing_data.head(18) 
missing(df1) 
for col in df1.columns: 
    if df1[col].isnull().mean()*100>40: 
        df1.drop(col,axis=1,inplace=True) 
df1 
df1.columns
df1.dtypes.value_counts() 
f = lambda x: x.median() if np.issubdtype(x.dtype, np.number) else x.mode().iloc[0] 
df1 = df1.fillna(df1.groupby('date').transform(f)) 
df1 
for col in df1.columns: 
    if df1[col].dtypes != object: 
        q1 = df1[col].quantile(0.25) 
        q2 = df1[col].quantile(0.50) 
        q3 = df1[col].quantile(0.75) 
        IQR = q3 - q1 
        llp = q1-1.5*IQR 
        ulp = q3+1.5*IQR 
        print('column name',col) 
        print('q1',q1) 
        print('q2',q2) 
        print('q3',q3)
        print('IQR',IQR) 
        print('llp',llp) 
        print('ulp',ulp) 
        print('mean:',df1[col].mean()) 
        print('median:',df1[col].median()) 
        print('mode',df1[col].mode()[0]) 
        print('skewness:',df1[col].skew()) 
        print('kurtosis:',df1[col].kurtosis()) 
        print('std',df1[col].std()) 
        print('max',df1[col].max()) 
        print('min',df1[col].min()) 
        print('null_value count:',df1[col].isnull().sum()) 
        print('\n') 
df1['floors'].unique() 
df1['bedrooms'].unique() 
Q1 = df1.quantile(0.25) 
Q3 = df1.quantile(0.75) 
IQR = Q3 â€“ Q1 
print('outliers count of each columns') 
((df1 < (Q1 - 1.5 * IQR)) | (df1 > (Q3 + 1.5 * IQR))).sum() 
df1.dtypes 
le=LabelEncoder() 
for col in df1.columns: 
    if df1[col].dtypes == object: 
        df1[col]= le.fit_transform(df1[col]) 
df1.ndim 
y = np.log(df1.price) 
X = df1.drop(['price'], axis=1) 
print(np.any(np.isnan(X))) 
data = np.nan_to_num(X) 
print(data) 
df1.ndim 
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.25,random_state=42) 
X=df1.drop('price',axis=1) 
y=df1['price']
df1 
a1=df1['date'] 
a2=df1['price'] 
a3=df1['bedrooms'] 
a4=df1['bathrooms'] 
a5=df1['sqft_living'] 
a6=df1['sqft_lot'] 
a7=df1['floors'] 
a8=df1['waterfront'] 
a9=df1['view'] 
a10=df1['condition'] 
a11=df1['sqft_above'] 
a12=df1['sqft_basement'] 
a13=df1['yr_built'] 
a14=df1['yr_renovated'] 
a15=df1['street'] 
a16=df1['city'] 
a17=df1['statezip'] 
a18=df1['country'] 
df1 = np.stack((a1,a2,a3,a4,a5,a6,a7,a8,a9,a10,a11,a12,a13,a14,a15,a16,a17,a18),axis=1 
df1 = np.hstack((a1, a2)) 
print(df1.shape) 
from sklearn.impute import SimpleImputer 
imp_mean = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0) 
imputer = imp_mean.fit([df1]) 
df1 = imputer.transform([df1]) 
print(df1) 
def train_models(X_train, y_train) 
 #use Decision Tree 
tree = DecisionTreeRegressor(max_features=75,max_depth=4, random_state = 0) 
    tree.fit(X_train, y_train) 
    y_pred_tree = tree.predict(X_test) 
  #use the RandomForestRegressor
   rf = RandomForestRegressor(n_estimators = 100,max_features =75, random_state = 0) 
    rf.fit(X_train, y_train) 
    y_pred_rf= rf.predict(X_test) 
  # use the support vector regressor 
    #from sklearn.svm import SVR 
    svr= SVR(kernel = 'rbf') 
    svr.fit(X_train, y_train) 
    y_pred_svr = svr.predict(X_test) 
    #from sklearn.svm import SVR 
    svr_l= SVR(kernel = 'linear') 
    svr_l.fit(X_train, y_train) 
    y_pred_svr_linear = svr_l.predict(X_test) 
  # use the knn regressor 
    knn = neighbors.KNeighborsRegressor() 
    knn.fit(X_train, y_train) 
    y_pred_knn = knn.predict(X_test) 
  # metrics of decision tree regressor 
  eanAbErr_tree= metrics.mean_absolute_error(y_test, y_pred_tree) 
    meanSqErr_tree= metrics.mean_squared_error(y_test, y_pred_tree) 
    rootMeanSqErr_tree= np.sqrt(metrics.mean_squared_error(y_test, y_pred_tree)) 
  # metrics of random forest regressor 
    meanAbErr_rf= metrics.mean_absolute_error(y_test, y_pred_rf) 
    meanSqErr_rf= metrics.mean_squared_error(y_test, y_pred_rf) 
    rootMeanSqErr_rf= np.sqrt(metrics.mean_squared_error(y_test, y_pred_rf)) 
  # metrics of knn regressor 
    meanAbErr_knn = metrics.mean_absolute_error(y_test, y_pred_knn) 
    meanSqErr_knn = metrics.mean_squared_error(y_test, y_pred_knn) 
    rootMeanSqErr_knn= np.sqrt(metrics.mean_squared_error(y_test, y_pred_knn))  
  # metrics of svr regressor 
    meanAbErr_svr = metrics.mean_absolute_error(y_test, y_pred_svr_linear) 
    meanSqErr_svr = metrics.mean_squared_error(y_test, y_pred_svr_linear) 
#print the tranning accurancy of each model 
    print('[1]Decision Tree Training Accurancy: ', r2_score(y_test,y_pred_tree)) 
    print('Mean Absolute Error:', meanAbErr_tree) 
    print('Mean Square Error:', meanSqErr_tree) 
    print('Root Mean Square Error:', rootMeanSqErr_tree) 
    print('\t') 
    print('[2]RandomForestRegressor Training Accurancy: ',r2_score(y_test,y_pred_rf)) 
    print('Mean Absolute Error:', meanAbErr_rf) 
    print('Mean Square Error:', meanSqErr_rf) 
    print('Root Mean Square Error:', rootMeanSqErr_rf) 
    print('\t')     
    print('[3]SupportvectorRegression Accuracy(rbf): ', r2_score(y_test,y_pred_svr)) 
    print('\t') 
    print('[4]SupportvectorRegression Accuracy(linear): ',  
r2_score(y_test,y_pred_svr_linear)) 
    rootMeanSqErr_svr= np.sqrt(metrics.mean_squared_error(y_test,y_pred_svr_linear))  
    print('Mean Absolute Error:', meanAbErr_svr) 
    print('Mean Square Error:', meanSqErr_svr) 
    print('Root Mean Square Error:', rootMeanSqErr_svr) 
    print('\t') 
    print('[5]knn Training Accurancy: ', r2_score(y_test,y_pred_knn)) 
    print('Mean Absolute Error:', meanAbErr_knn) 
    print('Mean Square Error:', meanSqErr_knn) 
    print('Root Mean Square Error:', rootMeanSqErr_knn) 
    print('\t') 
train_models(X_train, y_train) 
from sklearn.impute import SimpleImputer 
imp = SimpleImputer(missing_values=np.nan, strategy='mean') 
imp.fit([[1, 2], [np.nan, 3], [7, 6]]) 
X = [[np.nan, 2], [6, np.nan], [7, 6]] 
print(imp.transform(X)) 
from sklearn.linear_model import LinearRegression 
mlr = LinearRegression()   
mlr.fit(X_train, y_train) 
y_pred_mlr= mlr.predict(X_test) 
y_pred_mlr 
r2_mlr =r2_score(y_test,y_pred_mlr) 
print('r2_score:',r2_mlr*100) 
import pickle 
with open('model.pkl','wb') as files: 
    pickle.dump(mlr,files) 
